{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Main file project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold \n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os \n",
    "import pandas as pd \n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "453797it [00:01, 416587.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset (with the labels)\n",
    "X = []\n",
    "y = []\n",
    "with open(\"./../Data/training.txt\", \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.split()\n",
    "        X.append(np.array([int(line[0]), int(line[1])]))\n",
    "        y.append(np.array(int(line[2])))\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node_info = [] \n",
    "\n",
    "i =0\n",
    "j=0\n",
    "for root, dirs, files in os.walk(\"./../Data/node_information/text\", topdown=False):\n",
    "        for name in tqdm(files):\n",
    "            path = os.path.join(\"./../Data/node_information/text\",name)\n",
    "            try:\n",
    "                f = open(path, \"r\",  encoding='utf-8', errors='ignore')\n",
    "                Node_info.append(f.read())\n",
    "            except:\n",
    "                print(path)\n",
    "                i+=1\n",
    "            j+=1\n",
    "\n",
    "print(i/j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node_info = [] \n",
    "\n",
    "for nd in tqdm(X):\n",
    "        f = open(\"./../Data/node_information/text/\"+str(nd[0])+\".txt\", \"r\",  encoding='utf-8', errors='ignore')\n",
    "        a= f.read()\n",
    "        f = open(\"./../Data/node_information/text/\"+str(nd[1])+\".txt\", \"r\",  encoding='utf-8', errors='ignore')\n",
    "        a=a+f.read()\n",
    "        Node_info.append(a)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill graph from np.arrays\n",
    "def fill_graph(X, y):\n",
    "    G = nx.Graph()\n",
    "    for nd, v in tqdm(zip(X, y)):\n",
    "        if int(v) == 1:\n",
    "            G.add_edge(nd[0], nd[1])\n",
    "    return G\n",
    "\n",
    "def node2vec_input(X, y):       \n",
    "    with open('./../node2vec/data.txt', 'w') as outfile:\n",
    "        for nd, v in tqdm(zip(X, y)):\n",
    "            if int(v) == 1:\n",
    "                outfile.write(str(nd[0])+ \" \"+str(nd[1])+\"\\n\")\n",
    "\n",
    "\n",
    "def export_to_json(X,y):\n",
    "    data = {}\n",
    "    data[\"edges\"]=[]\n",
    "    data[\"nodes\"]=[]\n",
    "    i=0\n",
    "    for _ in tqdm(X):\n",
    "        data[\"nodes\"].append({\n",
    "            'id': str(_[0]),\n",
    "            'label' : str(_[0]),\n",
    "            'group' : 1\n",
    "        })\n",
    "        data[\"nodes\"].append({\n",
    "            'id': str(_[1]),\n",
    "            'label' : str(_[1]),\n",
    "            'group' : 1\n",
    "        })\n",
    "        i+=1\n",
    "    \n",
    "    for nd, v in tqdm(zip(X, y)):\n",
    "        if int(v) == 1:\n",
    "            data['edges'].append({\n",
    "                'from': str(nd[0]),\n",
    "                'to' : str(nd[1])\n",
    "            })\n",
    "            \n",
    "    with open('./Vis/data.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    \n",
    "\n",
    "# Compute jaccard predictions given a Graph and the nodes array\n",
    "def jaccard_prediction(G, X):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        try:\n",
    "            coef = [[u, v, p] for u, v, p in nx.jaccard_coefficient(G, [(x[0], x[1])])][0]\n",
    "        except KeyError:  # If the node tryed isn't in the Graph we predict 0...\n",
    "            coef = [0, 0, 0]\n",
    "        if coef[2] > 0.005:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Compute the score given a predictor and the training set\n",
    "\n",
    "def computeTrainingScore_kFold(predictor, X, y, n_splits):\n",
    "    kf = KFold(n_splits = n_splits)  # Define the split - into n_splits folds\n",
    "\n",
    "    mean_score = 0\n",
    "    i=1\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Let's reinitilize the graph\n",
    "        G = fill_graph(X_train, y_train)\n",
    "        y_pred = predictor(G, X_val)\n",
    "        mean_score += compute_score(y_pred, y_val)\n",
    "        print(mean_score/i)\n",
    "        i+=1\n",
    "    return mean_score/n_splits\n",
    "\n",
    "def computeTrainingScore(predictor, X, y, test_size=0.1):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    G = fill_graph(X_train, y_train)\n",
    "    y_pred = predictor(G, X_val)\n",
    "    return compute_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G= fill_graph(X,y)\n",
    "export_to_json(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=[]\n",
    "for nd, v in tqdm(zip(X, y)):\n",
    "    if int(v) == 1:\n",
    "        Z.append(np.array([nd[0], nd[1]]))\n",
    "Z= np.array(Z)\n",
    "np.savetxt(\"data.edgelist\", Z, delimiter=\" \",  fmt=\"%d\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only NLP, supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data (bag of words + TFIDF transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(Node_info)\n",
    "nodes_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nodes_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing cossine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av1=0\n",
    "av2=0\n",
    "i,j=0,0\n",
    "d=0\n",
    "for nd, v in tqdm(zip(X, y)):\n",
    "    if int(v) ==1:\n",
    "        av1+=np.dot(np.transpose(nodes_tfidf[:,nd[0]]),nodes_tfidf[:,nd[1]])\n",
    "        i+=1\n",
    "    else:\n",
    "        av2+=np.dot(np.transpose(nodes_tfidf[:,nd[0]]),nodes_tfidf[:,nd[1]])\n",
    "        j+=1\n",
    "    d+=1\n",
    "    if(d==100): break;\n",
    "print(\"Cos for linked edges : \" + (str(av1/i)))\n",
    "print(\"Cos for linked edges : \" + (str(av2/j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.concatenate((Node_info[X[:,0][0]],Node_info[X[:,1][0]]), axis=0)\n",
    "print(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1, y, test_size=0.10, random_state=42)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2vec embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= fill_graph(X, y)\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=9, num_walks=5, workers=20, p=1, q=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit(window=3, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"node2vec_d64_wl9_nw5_w3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"node2vec_d64_wl9_nw5_w3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"model_d64_wl9_nw5_w3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('2'))  # Output node names are always strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - word2vec and Gensin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Node_info[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Node_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[0][1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df[0][1:2000])\n",
    "\n",
    "t=t = time()\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=8)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"Node_info_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"list_of_nodes\", Node_info, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import node2vec and basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33163it [00:00, 40634.50it/s]\n"
     ]
    }
   ],
   "source": [
    "T={}\n",
    "i=0\n",
    "with open(\"./model_d64_wl9_nw5_w5.txt\", \"r\") as f:\n",
    "     for line in tqdm(f):\n",
    "        if(i==0): pass\n",
    "        else:\n",
    "            line = line.split()\n",
    "            T[float(line[0])]=np.array([float(a) for a in (line[1:-1])])\n",
    "        i+=1\n",
    "#X= np.array(X[1:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7404842 , -1.4362556 ,  0.5302325 , -0.1030505 , -0.6509405 ,\n",
       "        0.07901148, -1.4621917 ,  0.06504963, -1.9926604 , -0.2518935 ,\n",
       "        0.4129057 , -0.48431596,  0.79965603,  0.5177307 , -0.49467507,\n",
       "       -0.08762395, -0.43693647, -0.12519674, -1.2167609 ,  0.15535897,\n",
       "       -1.185967  ,  0.49078524, -0.13660388,  0.2120918 ,  0.3679756 ,\n",
       "        0.3191221 , -0.04115665,  0.14868537,  0.10401659, -0.383966  ,\n",
       "        0.9506267 , -1.3403143 , -0.6030259 , -0.13534434, -1.4833179 ,\n",
       "        1.2220614 ,  0.79736674,  0.06079897, -0.8068173 ,  0.46822396,\n",
       "        1.9668282 ,  0.8494924 ,  1.5610387 ,  0.40520224,  1.3071319 ,\n",
       "        0.7966213 , -0.81597453,  1.370335  , -0.9699791 , -0.37755573,\n",
       "        0.31234258,  0.56113994, -0.31787762, -1.3367473 ,  0.50323224,\n",
       "       -0.73666686,  0.5041424 ,  1.4499984 , -0.22668447, -1.17377   ,\n",
       "       -0.42792922, -0.7352172 ,  1.4161828 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[10481]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadamard(x,y):\n",
    "    return x*y\n",
    "\n",
    "def WeightedL1(x,y):\n",
    "    return np.abs(x-y)\n",
    "\n",
    "def WeightedL2(x,y):\n",
    "    return (x-y)**2\n",
    "\n",
    "def avg(x,y):\n",
    "    return (x+y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "453797it [00:01, 233293.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.11567243e-01  7.51485626e-01  3.51972761e-01 ...  1.70197422e-01\n",
      "   5.39839552e-01  1.50590874e+00]\n",
      " [ 7.28803303e-03  1.22077349e-04  1.51414119e-02 ...  4.07585895e-03\n",
      "   1.71206881e-02  1.90837207e-02]\n",
      " [ 2.36823544e-01  2.29267084e+00  1.45157386e-02 ...  4.75318510e-01\n",
      "   5.03370516e-01  5.94864516e-01]\n",
      " ...\n",
      " [-1.10656749e-03  7.36422758e-04  2.98993045e-02 ...  2.21257402e-02\n",
      "   4.89522059e-02  3.54070333e-02]\n",
      " [-1.22020542e-03  5.00479629e-02  1.09216373e-01 ...  6.72271185e-01\n",
      "   1.72504766e-01  6.20401182e-01]\n",
      " [ 2.12931629e-03 -5.58029833e-04  1.15155316e-02 ... -6.24292242e-03\n",
      "   2.61171926e-02  2.63767048e-02]]\n"
     ]
    }
   ],
   "source": [
    "Z=[]\n",
    "yt=[]\n",
    "for nd, v in tqdm(zip(X, y)):\n",
    "    try:\n",
    "        Z.append(hadamard(T[nd[0]],T[nd[1]]))\n",
    "        yt.append(v)\n",
    "    except:\n",
    "        pass\n",
    "Z=np.array(Z)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Z, yt, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc= SVC().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=clf.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf= MultinomialNB().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=clf.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc= RandomForestClassifier().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=rfc.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc= ExtraTreesClassifier().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=etc.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "gbc= GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=gbc.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of training\n",
      "0.9367443490450325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn= KNeighborsClassifier().fit(X_train, y_train)\n",
    "print(\"end of training\")\n",
    "y_pred=knn.predict(X_test)\n",
    "print(f1_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers classificator with softmax and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    keras.layers.Dense(128, input_shape=(64,), activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"softmax\")\n",
    "]\n",
    "\n",
    "model = keras.Sequential(layers)\n",
    "model.summary()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=50, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,7))\n",
    "    axs[0].plot(history.epoch,history.history[\"loss\"])\n",
    "    axs[0].plot(history.epoch,history.history[\"val_loss\"])\n",
    "    axs[0].grid()\n",
    "    \n",
    "    axs[1].plot(history.epoch,history.history[\"accuracy\"])\n",
    "    axs[1].plot(history.epoch,history.history[\"val_accuracy\"])\n",
    "    axs[1].grid()\n",
    "    plt.show()\n",
    "        \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1) Given approach - Graph baseline - Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For training purpose, as the testing.txt set doesn't have the labels\n",
    "# We will split our original (training) dataset with kfolds \n",
    "# For each split we'll initilize a Graph, train our model and get the mean score\n",
    "# On the training set\n",
    "n_splits = 2\n",
    "\n",
    "jacc_score = computeTrainingScore(jaccard_prediction, X, y)\n",
    "print(jacc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
